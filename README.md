[![Python CI](https://github.com/Lucky015-7/FastAPI-Streamlit-RAG-Application/actions/workflows/ci.yml/badge.svg)](https://github.com/Lucky015-7/FastAPI-Streamlit-RAG-Application/actions/workflows/ci.yml)
ğŸš€ Multi-User RAG Chatbot

Production-Grade Document Intelligence with FastAPI, Streamlit & Groq

A high-performance, cloud-proven Retrieval-Augmented Generation (RAG) system enabling real-time, multi-user conversations with PDF and DOCX documents.
Built using a modular, containerized architecture and powered by Groq + LLaMA-3.3-70B for ultra-low-latency inference.

âœ¨ Core Features

âš¡ Ultra-Fast Inference
Leveraging Groqâ€™s LLaMA-3.3-70B for near-instant, context-aware responses.

ğŸ“‚ Multi-Format Document Intelligence
Upload, embed, and chat with PDF & DOCX files seamlessly.

ğŸ§  Persistent Conversational Memory
SQLite-backed chat history enables coherent multi-turn dialogue across sessions.

ğŸ”’ Privacy-First Vector Search
Local HuggingFace embeddings (all-MiniLM-L6-v2) with a self-hosted ChromaDB vector store.

ğŸ³ Fully Containerized
Orchestrated with Docker Compose for reproducible, environment-safe deployments.

â˜ï¸ Cloud-Proven on AWS
Successfully deployed and tested on AWS EC2 with production networking.

ğŸ› ï¸ Developer-Friendly API
Interactive Swagger UI for instant backend testing and exploration.

ğŸ—ï¸ Architecture Overview

The application is split into two independent services for scalability and clean separation of concerns:

/api  â†’ FastAPI Backend
       â€¢ Document ingestion & chunking
       â€¢ Vector store management
       â€¢ RAG chain orchestration

/app  â†’ Streamlit Frontend
       â€¢ Document upload & management
       â€¢ Real-time conversational UI

ğŸ³ Run with Docker (Recommended)

Docker Compose is the recommended way to run the application, handling networking and dependencies automatically.

1ï¸âƒ£ Prerequisites

Docker & Docker Compose

Groq API Key â†’ https://console.groq.com

2ï¸âƒ£ Configuration

Create a .env file in the project root:

GROQ_API_KEY=your_groq_api_key_here

# Optional: LangSmith Tracing
LANGCHAIN_API_KEY=your_langchain_api_key_here
LANGCHAIN_TRACING_V2=true
LANGCHAIN_PROJECT=Multi-User-RAG-App

3ï¸âƒ£ Launch Services
docker compose up -d --build


ğŸ”¹ FastAPI Backend: http://localhost:8000

ğŸ”¹ Swagger Docs: http://localhost:8000/docs

ğŸ”¹ Streamlit UI: http://localhost:8501

â˜ï¸ AWS EC2 Deployment
Security Group (Inbound Rules)

Allow the following ports (0.0.0.0/0):

Port	Purpose
8000	FastAPI Backend
8501	Streamlit Frontend
22	SSH Access
Deployment Commands
git clone https://github.com/Lucky015-7/FastAPI-Streamlit-RAG-Application.git
cd FastAPI-Streamlit-RAG-Application
docker compose up -d --build


Your RAG chatbot is now live on AWS ğŸš€

ğŸ› ï¸ Manual Installation (Local Testing)

âš ï¸ Run commands from the project root to support absolute imports (e.g. from api.utils...)

Install Dependencies
pip install -r requirements.txt

Terminal 1 â€” Backend
uvicorn api.main:app --reload

Terminal 2 â€” Frontend
streamlit run app/streamlit_app.py

ğŸ“ Project Structure
â”œâ”€â”€ api/                     # FastAPI Backend Package
â”‚   â”œâ”€â”€ main.py              # Entry point (api.main:app)
â”‚   â”œâ”€â”€ langchain_utils.py   # RAG chains & logic
â”‚   â”œâ”€â”€ chroma_utils.py      # Vector store utilities
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ app/                     # Streamlit Frontend
â”‚   â”œâ”€â”€ streamlit_app.py     # UI & API integration
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ docker-compose.yml       # Multi-service orchestration
â”œâ”€â”€ requirements.txt         # Dependencies (NumPy < 2.0 pinned)
â””â”€â”€ rag_app.db               # SQLite chat history

ğŸ§  Tech Stack

LLMs & AI: Groq (LLaMA-3.3), LangChain, HuggingFace, ChromaDB
Backend: Python, FastAPI, Uvicorn, SQLite
Frontend: Streamlit
DevOps & Cloud: Docker, Docker Compose, AWS EC2, Ubuntu Linux

ğŸ“„ License

Licensed under the MIT License.


